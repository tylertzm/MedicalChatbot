
import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import Dataset, DataLoader
import json
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch.nn.functional as F
import openai
import os

# Check if MPS is available
device = (
    torch.device("mps")
    if torch.backends.mps.is_available()
    else torch.device("cpu")
)
print(f"Using device: {device}")

# Initialize OpenAI API (You'll need your OpenAI API key here)
openai.api_key = secretsfile 

class SimpleTokenizer:
    def __init__(self):
        self.word2idx = {'<pad>': 0, '<unk>': 1}
        self.idx2word = {0: '<pad>', 1: '<unk>'}
        self.vocab_size = 2

    def fit(self, texts):
        words = set()
        for text in texts:
            words.update(self.tokenize(text))
        
        for word in words:
            if word not in self.word2idx:
                self.word2idx[word] = self.vocab_size
                self.idx2word[self.vocab_size] = word
                self.vocab_size += 1

    def tokenize(self, text):
        # Simple tokenization by splitting on spaces and removing punctuation
        text = re.sub(r'[^\w\s]', '', text.lower())
        return text.split()

class MedicalDataset(Dataset):
    def __init__(self, X, y, tokenizer):
        self.X = X
        self.y = y
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        text = self.X[idx]
        label = self.y[idx]
        
        # Tokenize and pad/truncate to fixed length
        tokens = self.tokenizer.tokenize(text)
        if len(tokens) > 100:  # Truncate
            tokens = tokens[:100]
        else:  # Pad
            tokens = tokens + ['<pad>'] * (100 - len(tokens))
            
        return {
            'text': torch.tensor([self.tokenizer.word2idx.get(t, 0) for t in tokens]),
            'label': torch.tensor(label)
        }

class MedicalChatbot(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        # Use the last hidden state
        lstm_out = lstm_out[:, -1, :]
        return self.fc(lstm_out)

class ChatbotTrainer:
    def __init__(self, intents_file):
        self.load_data(intents_file)
        self.setup_model()
        
    def load_data(self, intents_file):
        # Load and parse the JSON file
        with open(intents_file, 'r', encoding='utf-8') as f:
            self.intents = json.load(f)

        # Prepare training data
        X = []
        y = []
        self.tag_to_responses = {}
        
        for intent in self.intents['intents']:
            self.tag_to_responses[intent['tag']] = intent['responses']
            for pattern in intent['patterns']:
                X.append(pattern)
                y.append(intent['tag'])
        
        # Encode labels
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(y)
        
        # Split data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Initialize and fit tokenizer
        self.tokenizer = SimpleTokenizer()
        self.tokenizer.fit(X)
        
        # Create datasets
        self.train_dataset = MedicalDataset(self.X_train, self.y_train, self.tokenizer)
        self.test_dataset = MedicalDataset(self.X_test, self.y_test, self.tokenizer)
        
    def setup_model(self):
        # Model parameters
        self.embedding_dim = 100
        self.hidden_dim = 64
        num_classes = len(self.label_encoder.classes_)
        
        self.model = MedicalChatbot(
            self.tokenizer.vocab_size,
            self.embedding_dim,
            self.hidden_dim,
            num_classes
        ).to(device)
        
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.model.parameters())

    def train(self, epochs=10, batch_size=32, checkpoint_path="chatbot_checkpoint.pth", accuracy_threshold=97.0):
        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0

            for batch in train_loader:
                texts = batch['text'].to(device)
                labels = batch['label'].to(device)

                self.optimizer.zero_grad()
                outputs = self.model(texts)
                loss = self.criterion(outputs, labels)

                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

                # Calculate accuracy
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            avg_loss = total_loss / len(train_loader)
            accuracy = 100 * correct / total
            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')

            # Save checkpoint after every epoch
            self.save_checkpoint(epoch, checkpoint_path)

            # Check if accuracy has reached the threshold
            if accuracy >= accuracy_threshold:
                print(f"Accuracy has reached {accuracy_threshold}%. Stopping training early.")
                break  # Exit the loop if accuracy is sufficient

    def save_checkpoint(self, epoch, path="chatbot_checkpoint.pth"):
        """Save model and optimizer state during training"""
        checkpoint = {
            'epoch': epoch,
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'loss': self.criterion,
            'label_encoder': self.label_encoder,
            'tokenizer': self.tokenizer,
            'tag_to_responses': self.tag_to_responses
        }
        torch.save(checkpoint, path)
        print(f"Checkpoint saved to {path}")

    def load_checkpoint(self, path="chatbot_checkpoint.pth", intents_file=None):
        """Load checkpoint to resume training, and reinitialize data if necessary"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state'])
        self.criterion = checkpoint['loss']
        self.label_encoder = checkpoint['label_encoder']
        self.tokenizer = checkpoint['tokenizer']
        self.tag_to_responses = checkpoint['tag_to_responses']
        epoch = checkpoint['epoch']
        
        # If intents_file is provided, load data again
        if intents_file:
            self.load_data(intents_file)  # Re-load data (if necessary)
        
        print(f"Checkpoint loaded from {path}, Resuming from epoch {epoch}")
        return epoch  # Return the epoch number so training can continue

    def predict(self, user_input):
        """Use GPT to refine the user's input and predict the intent"""
        # Attempt to improve the user input using GPT for better clarity
        refined_input = self.refine_input_with_gpt(user_input)
        
        # Tokenize and predict intent
        tokens = self.tokenizer.tokenize(refined_input)
        tokens = tokens + ['<pad>'] * (100 - len(tokens))  # Pad/truncate
        token_tensor = torch.tensor([self.tokenizer.word2idx.get(t, 1) for t in tokens]).unsqueeze(0).to(device)

        self.model.eval()
        with torch.no_grad():
            output = self.model(token_tensor)
            _, predicted_idx = torch.max(output, 1)
            predicted_tag = self.label_encoder.inverse_transform([predicted_idx.item()])[0]
            confidence = F.softmax(output, dim=1)[0][predicted_idx].item()

        # If confidence is low, use GPT for fallback
        if confidence < 0.6:  # Threshold for confidence
            response = self.get_gpt_response(user_input)
        else:
            response = np.random.choice(self.tag_to_responses[predicted_tag])

        return response, confidence

    def refine_input_with_gpt(self, user_input):
        """Use GPT to refine or clarify the user's input."""
        try:
            refined_input = openai.ChatCompletion.create(
                model="gpt-4",  # Using the new GPT-4 model
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Rephrase or clarify the following input for better understanding: {user_input}"}
                ]
            )
            return refined_input['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"Error while refining input with GPT: {e}")
            return user_input  # If GPT fails, return the original input

    def get_gpt_response(self, user_input):
        """Use GPT for fallback response when confidence is low."""
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",  # Using the new GPT-4 model
                messages=[
                    {"role": "system", "content": "You are a helpful medical chatbot."},
                    {"role": "user", "content": user_input}
                ]
            )
            return response['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"Error while getting GPT response: {e}")
            return "Sorry, I couldn't understand your question. Could you clarify?"

# Example usage
trainer = ChatbotTrainer('/Users/zhen-meitan/Desktop/Personal/Uni/Projektstudium/BertModel/intents.json')
trainer.train(epochs=400)
